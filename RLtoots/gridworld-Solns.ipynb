{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Gridworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tut_basics as tb\n",
    "\n",
    "from plot_fxns import plotWorld, plotStateValue, plotStateActionValue, plotPolicyPi, plotGreedyPolicyQ\n",
    "from rl_agents import RLAgent, RLExampleAgent\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Build A Gridworld with Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# build and plot world\n",
    "world = tb.GridWorldExample1()\n",
    "plotWorld(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# initialize an agent and run an episode (see program printout)\n",
    "agent = RLExampleAgent(world)\n",
    "tb.run_episode(agent, print_details = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot value functions\n",
    "plotStateValue(agent.v, world)\n",
    "plotStateActionValue(agent.q, world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot policy map\n",
    "plotPolicyPi(agent.Ppi, world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Dynamic Programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and plot a new world\n",
    "world = tb.GridWorldExample2()\n",
    "plotWorld(world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP_Agent(RLAgent):\n",
    "    \"\"\"Dynamic Programming (DP) agent.\"\"\"\n",
    "\n",
    "    def initRandomPolicy(self):\n",
    "        \"\"\"\n",
    "        Build the matrix P_pi = sum_a pi(a|s)*Pr_a(s'|s)\n",
    "        (See tutorial slides 13-15)\"\"\"\n",
    "        Psum = self.world.P.sum(axis=0)\n",
    "        Pnorm = Psum.sum(axis=1)\n",
    "        zero_idxs = Pnorm==0.0\n",
    "        Pnorm[zero_idxs] = 1.0\n",
    "        self.P_pi = (Psum.T / Pnorm).T\n",
    "\n",
    "    def evaluatePolicy(self, gamma):\n",
    "        delta = 1\n",
    "        maxiters = 1000  # maximum number of iterations\n",
    "        itr = 0\n",
    "        while(delta > 0.001 and itr < maxiters):\n",
    "            itr += 1\n",
    "            v_new = self.world.R + gamma * self.P_pi.dot(self.v) # note the policy doesn't appear here! It does in pseudocode. but has already been 'applied' to get P_pi\n",
    "            delta = np.max(np.abs(v_new - self.v))\n",
    "            self.v = v_new\n",
    "\n",
    "    def improvePolicy(self):\n",
    "        self.P_pi = np.zeros((self.world.nstates,self.world.nstates))\n",
    "        for s in range(self.world.nstates):\n",
    "            transitions = np.sum(self.world.P[:,s,:],axis=0).astype(bool)\n",
    "            nextvals = np.full((self.world.nstates,),-1e6)\n",
    "            nextvals[transitions] = self.v[transitions]\n",
    "            s_next = np.argmax(nextvals)\n",
    "            self.P_pi[s,s_next] = 1\n",
    "\n",
    "    def policyIteration(self, gamma):\n",
    "        print(\"Running policy iteration...\")\n",
    "        policyStable = False\n",
    "        itr = 0\n",
    "        maxiters = 1000\n",
    "        while(not policyStable and itr < maxiters):\n",
    "            itr += 1\n",
    "            Ppi_old = self.P_pi\n",
    "            self.evaluatePolicy(gamma)\n",
    "            self.improvePolicy()\n",
    "            policyStable = np.array_equal(Ppi_old,self.P_pi)\n",
    "        print(\"Converged after %d iterations.\" % itr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DP_Agent(world)\n",
    "# set your discount factor\n",
    "gamma = 0.99\n",
    "# initialize agent with random policy\n",
    "agent.initRandomPolicy()\n",
    "# do policy evaluation and improvment\n",
    "agent.policyIteration(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot value function\n",
    "plotStateValue(agent.v, world)\n",
    "# plot policy map\n",
    "plotPolicyPi(agent.P_pi, world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDSarsa_Agent(RLAgent):\n",
    "    def __init__(self, world):\n",
    "        super(TDSarsa_Agent, self).__init__(world)\n",
    "        self.policy = self.epsilongreedyQPolicy\n",
    "\n",
    "    def evaluatePolicyQ(self, gamma, alpha, ntrials):\n",
    "        delta = 1.0\n",
    "        old_q = self.q\n",
    "        for i in range(ntrials):\n",
    "            is_terminal = False\n",
    "            c = 0\n",
    "            self.reset()\n",
    "            s = self.state\n",
    "            a = self.choose_action()\n",
    "            while not is_terminal:\n",
    "                c += 1\n",
    "                is_terminal = self.take_action(a)\n",
    "                a_prime = self.choose_action() if not is_terminal else 'D'\n",
    "                self.q[s,self.action_dict[a]] += alpha*(self.reward + gamma*self.q[self.state,self.action_dict[a_prime]] - self.q[s,self.action_dict[a]])\n",
    "                s = self.state\n",
    "                a = a_prime\n",
    "            delta = min(np.max(np.abs(self.q - old_q)),delta)\n",
    "            old_q = self.q\n",
    "\n",
    "    def policyIteration(self, gamma, alpha, ntrials):\n",
    "        print(\"Running TD policy iteration...\")\n",
    "        policyStable = False\n",
    "        itr = 0\n",
    "        maxiters = 1000 # catch the while loop.\n",
    "        oldA = np.zeros((self.world.nstates,))\n",
    "        while (not policyStable and itr < maxiters):\n",
    "            itr += 1\n",
    "            self.evaluatePolicyQ(gamma, alpha, ntrials)   \n",
    "            policyStable = np.array_equal(oldA,np.argmax(self.q, axis=1)) # see if policy changes!\n",
    "            oldA = np.argmax(self.q, axis=1)\n",
    "        print(\"Converged after {} iterations.\".format(itr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TD policy iteration...\n",
      "Converged after 2 iterations.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "world = tb.GridWorldExample3()\n",
    "agent = TDSarsa_Agent(world)\n",
    "\n",
    "gamma = 0.99\n",
    "alpha = 0.2\n",
    "ntrials = 500\n",
    "agent.evaluatePolicyQ(gamma, alpha, ntrials)\n",
    "agent.policyIteration(gamma, alpha, ntrials)\n",
    "\n",
    "plotStateActionValue(agent.q,world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Off-Policy Temporal Difference (TD) -- Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you will compare Q learning with SARSA from above. Which one is better, and why? Plot the Q-values and policies for the two. Run 100 episodes of each after learning, and compare average total return for each episode. Which algorithm receives more reward on average? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDQ_Agent(TDSarsa_Agent):\n",
    "    def __init__(self, world):\n",
    "        super(TDQ_Agent, self).__init__(world)\n",
    "        self.policy = self.epsilongreedyQPolicy\n",
    "        self.offpolicy = self.greedyQPolicy\n",
    "\n",
    "    def choose_offpolicy_action(self):\n",
    "        state = self.world.get_state()\n",
    "        actions = self.world.get_actions()\n",
    "        self.action = self.offpolicy(state, actions)\n",
    "        return self.action\n",
    "\n",
    "    def evaluatePolicyQ(self, gamma, alpha, ntrials):\n",
    "        delta = 1.0\n",
    "        old_q = self.q\n",
    "        for i in range(ntrials):\n",
    "            is_terminal = False\n",
    "            c = 0\n",
    "            self.reset()\n",
    "            s = self.state\n",
    "            a = self.choose_action()\n",
    "            while not is_terminal:\n",
    "                c += 1\n",
    "                is_terminal = self.take_action(a) # taking an action gives terminality status.\n",
    "           # explore from the epsilon-greedy policy \n",
    "                a_prime = self.choose_action() if not is_terminal else 'D'\n",
    "           # below we choose a prime from the TRUE policy! \n",
    "                a_greedy = self.choose_offpolicy_action() if not is_terminal else 'D'\n",
    "                self.q[s,self.action_dict[a]] += alpha*(self.reward + gamma*self.q[self.state,self.action_dict[a_greedy]] - self.q[s,self.action_dict[a]])\n",
    "                s = self.state\n",
    "                a = a_prime\n",
    "            delta = min(np.max(np.abs(self.q - old_q)),delta)\n",
    "            old_q = self.q\n",
    "        \n",
    "    def policyIteration(self, gamma, alpha, ntrials):\n",
    "        print(\"Running TD policy iteration...\")\n",
    "        policyStable = False\n",
    "        itr = 0\n",
    "        maxiters = 1000 # catch the while loop.\n",
    "        oldA = np.zeros((self.world.nstates,))\n",
    "        while (not policyStable and itr < maxiters):\n",
    "            itr += 1\n",
    "            self.evaluatePolicyQ(gamma, alpha, ntrials)   \n",
    "            policyStable = np.array_equal(oldA,np.argmax(self.q, axis=1)) # see if policy changes!\n",
    "            oldA = np.argmax(self.q, axis=1)\n",
    "        print(\"Converged after {} iterations.\".format(itr))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tut_basics import CliffWorld\n",
    "world = CliffWorld()\n",
    "# test Q Agent \n",
    "q_agent = TDQ_Agent(world)\n",
    "alpha = 0.05\n",
    "ntrials = 2000\n",
    "gamma = 0.9\n",
    "q_agent.policyIteration(gamma, alpha, ntrials)\n",
    "plotStateActionValue(q_agent.q,world)\n",
    "plotGreedyPolicyQ(q_agent.q,world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare control with Q learner against control with SARSA\n",
    "world = CliffWorld()\n",
    "sarsa_agent = TDSarsa_Agent(world)\n",
    "alpha = 0.2\n",
    "ntrials = 2000\n",
    "gamma = 0.9\n",
    "sarsa_agent.policyIteration(gamma, alpha, ntrials)\n",
    "plotStateActionValue(sarsa_agent.q,world)\n",
    "plotGreedyPolicyQ(sarsa_agent.q,world)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 TD Lambda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDSarsaLambda_Agent(TDSarsa_Agent):\n",
    "    def __init__(self, world, lamb):\n",
    "        super(TDSarsaLambda_Agent, self).__init__(world)\n",
    "        self.policy = self.epsilongreedyQPolicy\n",
    "        self.lamb = lamb\n",
    "\n",
    "    def evaluatePolicyQ(self, gamma, alpha, ntrials):\n",
    "        delta = 1.0\n",
    "        old_q = self.q\n",
    "        eligibility = np.zeros((self.world.nstates,5)) #  store elig traces\n",
    "        for i in range(ntrials):\n",
    "            is_terminal = False\n",
    "            c = 0\n",
    "            self.reset()\n",
    "            s = self.state\n",
    "            a = self.choose_action()\n",
    "            while not is_terminal:\n",
    "                c += 1\n",
    "                is_terminal = self.take_action(a) # this updates other stuff and returs terminality\n",
    "                a_prime = self.choose_action() if not is_terminal else 'D'\n",
    "                delta = self.reward + gamma*self.q[self.state,self.action_dict[a_prime]] - self.q[s,self.action_dict[a]]\n",
    "                eligibility[s,self.action_dict[a]] += 1.0\n",
    "                self.q[s,self.action_dict[a]] += alpha*(self.reward + gamma*self.q[self.state,self.action_dict[a_prime]] - self.q[s,self.action_dict[a]])\n",
    "                for s in range(self.world.nstates):\n",
    "                    for a in range(5):\n",
    "                        self.q[s,a] += alpha*delta*eligibility[s,a]\n",
    "                        eligibility[s,a] *= gamma*self.lamb\n",
    "                        s = self.state\n",
    "                        a = a_prime\n",
    "                        delta = min(np.max(np.abs(self.q - old_q)),delta)\n",
    "                        old_q = self.q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-8b47ef714284>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mntrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluatePolicyQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mntrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mplotStateActionValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4bbb8c3eb6fe>\u001b[0m in \u001b[0;36mevaluatePolicyQ\u001b[0;34m(self, gamma, alpha, ntrials)\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma_prime\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meligibility\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0meligibility\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlamb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tut_basics import GridWorldExample2\n",
    "\n",
    "world = GridWorldExample2()\n",
    "\n",
    "# 1. test policy evaluation for one trial (episode)\n",
    "lamb = 0.9\n",
    "agent = TDSarsaLambda_Agent(world, lamb)\n",
    "alpha = 0.05\n",
    "ntrials = 1\n",
    "gamma = 0.9\n",
    "agent.evaluatePolicyQ(gamma, alpha, ntrials)\n",
    "plotStateActionValue(agent.q,world)\n",
    "\n",
    "sarsa0_agent = TDSarsa_Agent(world)\n",
    "sarsa0_agent.evaluatePolicyQ(gamma, alpha, ntrials)\n",
    "plotStateActionValue(sarsa0_agent.q,world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. do policy iteration\n",
    "ntrials = 10\n",
    "agent.policyIteration(gamma, alpha, ntrials)\n",
    "plotStateActionValue(agent.q,world)\n",
    "plotGreedyPolicyQ(agent.q,world)\n",
    "\n",
    "sarsa0_agent.policyIteration(gamma, alpha, ntrials)\n",
    "plotStateActionValue(sarsa0_agent.q,world)\n",
    "plotGreedyPolicyQ(agent.q,world)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
