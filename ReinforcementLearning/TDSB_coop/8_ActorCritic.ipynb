{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Week 8 - Network Agents: Actor Critic Networks.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"4YmBcQrytMSD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616079027439,"user_tz":240,"elapsed":457,"user":{"displayName":"Annik Carson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghrwy2SzOHtgbFT4k-SP4JNYSMBAVnHobU7WyaqVg=s64","userId":"13111220454514414588"}},"outputId":"4365c6dd-1a4c-43cb-a8a7-2dfbfdee26c7"},"source":["import numpy as np \n","import torch\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.nn as nn # to handle layers\n","import torch.optim as optim # for optimizer\n","from collections import namedtuple\n","from torch.distributions import Categorical\n","import random\n","import time \n","\n","import matplotlib.pyplot as plt\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","\n","sys.path.insert(0,'/content/drive/My Drive/Colab Notebooks/Basics of Reinforcement Learning/') ## will have to change this path! \n","\n","# import premade Gridworlds from Week2 Tutorial\n","from pyfiles.GridworldEnvs import GridWorld, GridWorld4 #BoundedGridworld, EdgewrapGridworld, CliffWorld"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eVt4JDlriCy-"},"source":["class fully_connected_AC_network(nn.Module):\n","    def __init__(self, input_dims, fc1_dims, fc2_dims, output_dims, lr):\n","        super(fully_connected_AC_network,self).__init__()\n","        self.input_dims = input_dims\n","        self.fc1_dims   = fc1_dims\n","        self.fc2_dims   = fc2_dims\n","        self.output_dims= output_dims\n","\n","        self.fc1 = nn.Linear(*self.input_dims,self.fc1_dims)\n","        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n","        self.pol = nn.Linear(self.fc2_dims, self.output_dims)\n","        self.val = nn.Linear(self.fc2_dims, 1)\n","\n","        self.lr         = lr\n","        self.optimizer  = optim.Adam(self.parameters(), lr=self.lr)\n","\n","        self.temperature = 1\n","\n","        # need loss function?\n","\n","        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.to(self.device)\n","\n","    def forward(self,state):\n","        state = torch.Tensor(state)\n","        x = F.relu(self.fc1(state))\n","        x = F.relu(self.fc2(x))\n","\n","        value   = self.val(x)\n","        policy = F.softmax(self.pol(x)/self.temperature,dim=-1)\n","\n","        return policy, value\n","\n","\n","### transition cache for storing info about runs \n","class Transition_Cache():\n","    def __init__(self, cache_size):\n","        self.cache_size = cache_size\n","        self.transition_cache = []\n","        self.cache_cntr = 0\n","\n","    def store_transition(self, transition):\n","        if len(self.transition_cache) < self.cache_size:\n","            self.transition_cache.append(transition)\n","        else:\n","            self.transition_cache[self.cache_cntr] = transition\n","            self.cache_cntr += 1 if self.cache_cntr < self.cache_size else 0\n","\n","    # clear buffer - can be used for MC methods at end of episode\n","    def clear_cache(self):\n","        self.transition_cache = []\n","        self.cache_cntr = 0\n","\n","    # samples transitions - can be used for TD methods that require buffer\n","    def sample_transition_cache(self, batch_size):\n","        # get a list of random index numbers and sample the cache\n","        sample = random.sample(self.transition_cache, batch_size)\n","\n","        rewards, expected_values, next_states, terminals  = zip(*[(s.reward, s.expected_value, s.next_state, s.done) for s in sample])\n","\n","        return rewards, expected_values, next_states, terminals"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbdjbhCRiDpp"},"source":["Transition = namedtuple('Transition', 'episode, transition, state, action, reward, \\\n","                                next_state, log_prob, expected_value, target_value, done, readable_state')\n","\n","class Agent(object):\n","    def __init__(self, network, state_representations=[], **kwargs):\n","        self.MFC = network\n","        \n","        self.transition_cache = Transition_Cache(cache_size=10000)\n","\n","        self.optimizer = network.optimizer\n","\n","        self.gamma = kwargs.get('discount',0.98) # discount factor for return computation\n","\n","        self.get_action = self.MF_action\n","        self.counter = 0\n","        self.calc_loss = self.MC_loss\n","\n","        self.state_reps = state_representations\n","\n","    def get_state_representation(self,state):\n","        if self.state_reps == []:\n","            return state\n","        else:\n","            return self.state_reps[state]\n","\n","    def MF_action(self, state_observation):\n","        policy, value = self.MFC(state_observation)\n","\n","        a = Categorical(probs=policy,logits=None)\n","\n","        action = a.sample()\n","        \n","        return action.item(), a.log_prob(action), value.view(-1) ##TODO: why view instead of item\n","\n","    def MC_loss(self):\n","        # compute monte carlo return\n","        self.discount_rwds()\n","\n","        pol_loss = 0\n","        val_loss = 0\n","        for transition in self.transition_cache.transition_cache:\n","            G_t = transition.target_value\n","            V_t = transition.expected_value\n","            delta = G_t - V_t.item()\n","\n","            log_prob = transition.log_prob\n","            #print(\"comput loss for step:\", transition.readable_state, log_prob)\n","            pol_loss += -log_prob * delta\n","            G_t = torch.Tensor([G_t])\n","            v_loss = torch.nn.L1Loss()(V_t, G_t)\n","            val_loss += v_loss\n","        return pol_loss, val_loss\n","\n","\n","    def discount_rwds(self):\n","        transitions = self.transition_cache.transition_cache\n","\n","        running_add = 0\n","        returns = []\n","        for t in reversed(range(len(transitions))):\n","            running_add = running_add*self.gamma + transitions[t].reward\n","            transitions[t] = transitions[t]._replace(target_value = running_add)\n","\n","        self.transition_cache.transition_cache = transitions\n","\n","    def log_event(self, episode, event, state, action, reward, next_state, log_prob, expected_value, target_value, done, readable_state):\n","        # episode = trial\n","        # event = one step in the environment\n","        transition = Transition(episode=episode,\n","                                transition=event,\n","                                state=state,\n","                                action=action,\n","                                reward=reward,\n","                                next_state=next_state,\n","                                log_prob=log_prob,\n","                                expected_value=expected_value,\n","                                target_value=target_value,\n","                                done=done,\n","                                readable_state=readable_state\n","                                )\n","        self.transition_cache.store_transition(transition)\n","\n","    def update(self):\n","        pol_loss, val_loss = self.calc_loss()\n","\n","        self.optimizer.zero_grad()\n","        total_loss = pol_loss + val_loss\n","        total_loss.backward()\n","        self.optimizer.step()\n","\n","        return pol_loss, val_loss\n","\n","    def finish_(self):\n","        ## if monte carlo, call at end of trial\n","        ## if TD, call at end of event\n","        p, v = self.update()\n","   \n","        self.transition_cache.clear_cache()\n","        return p,v"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tuj4FHCeiSXE"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gonkZyfeirsO"},"source":["# make env\n","env = GridWorld(rows=10, cols=10, rewards={(3,3):10}, step_penalization=0, actionlist = ['Down', 'Up', 'Right', 'Left'], rewarded_action=None)\n","        \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hnB0w4gEjIXT"},"source":["input_dims = env.nstates\n","fc1_dims   = 200\n","fc2_dims   = 200\n","output_dims = env.action_space.n\n","network = fully_connected_AC_network(input_dims=[input_dims], fc1_dims=fc1_dims,\n","                                     fc2_dims=fc2_dims, output_dims=output_dims, lr =0.0005)\n","\n","\n","\n","oh_state_reps = []\n","for state in range(env.nstates):\n","  vec=np.zeros(env.nstates)\n","  vec[state]=1\n","  oh_state_reps.append(vec)\n","\n","ac_agent = Agent(network, state_representations=oh_state_reps)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"baHXdMKgkTPy","executionInfo":{"status":"ok","timestamp":1616079878560,"user_tz":240,"elapsed":951,"user":{"displayName":"Annik Carson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghrwy2SzOHtgbFT4k-SP4JNYSMBAVnHobU7WyaqVg=s64","userId":"13111220454514414588"}},"outputId":"183f6885-d837-4f6c-a016-40bc4ef82f7b"},"source":["index = 10\n","state = oh_state_reps[index]\n","action, log_prob, val = ac_agent.get_action(state)\n","print(action, log_prob, val)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2 tensor(-1.4432, grad_fn=<SqueezeBackward1>) tensor([0.0587], grad_fn=<ViewBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LT0L6Dugkcmp"},"source":["class expt(object):\n","\tdef __init__(self, agent, environment, **kwargs):\n","\t\tself.env = environment\n","\t\tself.agent = agent\n","\t\t# self.rep_learner = rep_learner  #TODO add in later\n","\t\tself.data = self.reset_data_logs()\n","\t\tself.agent.counter = 0\n","\n","\tdef reset_data_logs(self):\n","\t\tdata_log = {'total_reward': [],\n","\t\t\t\t\t'loss': [[], []],\n","\t\t\t\t\t'trial_length': [],\n","\t\t\t\t\t'EC_snap': [],\n","\t\t\t\t\t'P_snap': [],\n","\t\t\t\t\t'V_snap': []\n","\t\t\t\t\t}\n","\t\treturn data_log\n","\n","\tdef end_of_trial(self, trial):\n","\t\tp, v = self.agent.finish_()\n","\n","\t\tself.data['total_reward'].append(self.reward_sum) # removed for bootstrap expts\n","\t\tself.data['loss'][0].append(p)\n","\t\tself.data['loss'][1].append(v)\n","\n","\t\tself.running_rwdavg = np.mean(self.data['total_reward'][-self.print_freq:])\n","\n","\t\tif trial % self.print_freq == 0:\n","\t\t\tprint(f\"Episode: {trial}, Score: {self.reward_sum} (Running Avg:{self.running_rwdavg}) [{time.time() - self.t}s]\")\n","\t\t\tself.t = time.time()\n","\n","\tdef single_step(self,trial):\n","\t\t# get representation for given state of env. TODO: should be in agent to get representation?\n","\t\tstate_representation = self.agent.get_state_representation(self.state)\n","\t\treadable = 0\n","\n","\t\t# get action from agent\n","\t\taction, log_prob, expected_value = self.agent.get_action(state_representation)\n","\t\t# take step in environment\n","\t\tnext_state, reward, done, info = self.env.step(action)\n","\n","\t\t# end of event\n","\t\ttarget_value = 0\n","\t\tself.reward_sum += reward\n","\n","\t\tself.agent.log_event(episode=trial, event=self.agent.counter,\n","\t\t\t\t\t\t\t state=state_representation, action=action, reward=reward, next_state=next_state,\n","\t\t\t\t\t\t\t log_prob=log_prob, expected_value=expected_value, target_value=target_value,\n","\t\t\t\t\t\t\t done=done, readable_state=readable)\n","\t\tself.agent.counter += 1\n","\t\tself.state = next_state\n","\t\treturn done\n","\n","\tdef run(self, NUM_TRIALS, NUM_EVENTS, **kwargs): ### this is the main guy\n","\t\tself.print_freq = kwargs.get('printfreq', 100)\n","\t\tself.reset_data_logs()\n","\t\tself.t = time.time()\n","\n","\t\tfor trial in range(NUM_TRIALS):\n","\t\t\tself.state = self.env.reset()\n","\t\t\tself.reward_sum = 0\n","\n","\t\t\tfor event in range(NUM_EVENTS):\n","\t\t\t\tdone = self.single_step(trial)\n","\n","\t\t\t\tif done:\n","\t\t\t\t\tbreak\n","\n","\t\t\tself.end_of_trial(trial)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRpgkIyJx_WV"},"source":["ex = expt(ac_agent, env)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o14gKR5syDzf","executionInfo":{"status":"ok","timestamp":1616079953519,"user_tz":240,"elapsed":67828,"user":{"displayName":"Annik Carson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghrwy2SzOHtgbFT4k-SP4JNYSMBAVnHobU7WyaqVg=s64","userId":"13111220454514414588"}},"outputId":"328dbcba-6abd-4265-b9d1-b2a14c2e4b17"},"source":["ex.run(1000, 250, printfreq=10)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Episode: 0, Score: 10.0 (Running Avg:10.0) [0.0967099666595459s]\n","Episode: 10, Score: 10.0 (Running Avg:10.0) [0.8252239227294922s]\n","Episode: 20, Score: 10.0 (Running Avg:7.0) [1.4991486072540283s]\n","Episode: 30, Score: 0.0 (Running Avg:4.0) [2.0172371864318848s]\n","Episode: 40, Score: 10.0 (Running Avg:8.0) [1.3690168857574463s]\n","Episode: 50, Score: 10.0 (Running Avg:8.0) [1.1636242866516113s]\n","Episode: 60, Score: 10.0 (Running Avg:6.0) [1.6569738388061523s]\n","Episode: 70, Score: 0.0 (Running Avg:7.0) [1.7489540576934814s]\n","Episode: 80, Score: 0.0 (Running Avg:5.0) [1.4230546951293945s]\n","Episode: 90, Score: 10.0 (Running Avg:10.0) [0.8596746921539307s]\n","Episode: 100, Score: 10.0 (Running Avg:10.0) [0.8082914352416992s]\n","Episode: 110, Score: 10.0 (Running Avg:9.0) [1.5746724605560303s]\n","Episode: 120, Score: 10.0 (Running Avg:8.0) [1.5420253276824951s]\n","Episode: 130, Score: 10.0 (Running Avg:8.0) [1.12172269821167s]\n","Episode: 140, Score: 10.0 (Running Avg:10.0) [1.0375919342041016s]\n","Episode: 150, Score: 0.0 (Running Avg:8.0) [1.352079153060913s]\n","Episode: 160, Score: 10.0 (Running Avg:8.0) [1.0146143436431885s]\n","Episode: 170, Score: 10.0 (Running Avg:10.0) [0.8519508838653564s]\n","Episode: 180, Score: 0.0 (Running Avg:9.0) [1.0508642196655273s]\n","Episode: 190, Score: 10.0 (Running Avg:9.0) [0.7918927669525146s]\n","Episode: 200, Score: 10.0 (Running Avg:10.0) [0.39955615997314453s]\n","Episode: 210, Score: 10.0 (Running Avg:8.0) [1.0060808658599854s]\n","Episode: 220, Score: 10.0 (Running Avg:8.0) [1.2249996662139893s]\n","Episode: 230, Score: 10.0 (Running Avg:8.0) [1.3112165927886963s]\n","Episode: 240, Score: 10.0 (Running Avg:10.0) [0.912116527557373s]\n","Episode: 250, Score: 10.0 (Running Avg:10.0) [0.860478401184082s]\n","Episode: 260, Score: 10.0 (Running Avg:10.0) [0.9764690399169922s]\n","Episode: 270, Score: 10.0 (Running Avg:10.0) [0.8803234100341797s]\n","Episode: 280, Score: 10.0 (Running Avg:10.0) [0.7876465320587158s]\n","Episode: 290, Score: 10.0 (Running Avg:10.0) [0.8462357521057129s]\n","Episode: 300, Score: 10.0 (Running Avg:9.0) [1.2680869102478027s]\n","Episode: 310, Score: 10.0 (Running Avg:9.0) [0.7932162284851074s]\n","Episode: 320, Score: 10.0 (Running Avg:10.0) [0.9755740165710449s]\n","Episode: 330, Score: 10.0 (Running Avg:9.0) [1.1361024379730225s]\n","Episode: 340, Score: 10.0 (Running Avg:9.0) [0.8284971714019775s]\n","Episode: 350, Score: 10.0 (Running Avg:10.0) [0.7769911289215088s]\n","Episode: 360, Score: 10.0 (Running Avg:10.0) [1.2063119411468506s]\n","Episode: 370, Score: 10.0 (Running Avg:10.0) [0.7426149845123291s]\n","Episode: 380, Score: 10.0 (Running Avg:10.0) [1.0380289554595947s]\n","Episode: 390, Score: 10.0 (Running Avg:10.0) [0.9804732799530029s]\n","Episode: 400, Score: 10.0 (Running Avg:10.0) [0.3481447696685791s]\n","Episode: 410, Score: 10.0 (Running Avg:10.0) [0.7663435935974121s]\n","Episode: 420, Score: 10.0 (Running Avg:10.0) [0.614978551864624s]\n","Episode: 430, Score: 10.0 (Running Avg:10.0) [0.8070459365844727s]\n","Episode: 440, Score: 10.0 (Running Avg:10.0) [0.7205476760864258s]\n","Episode: 450, Score: 10.0 (Running Avg:10.0) [0.9886002540588379s]\n","Episode: 460, Score: 10.0 (Running Avg:10.0) [0.5264012813568115s]\n","Episode: 470, Score: 10.0 (Running Avg:10.0) [0.615286111831665s]\n","Episode: 480, Score: 10.0 (Running Avg:10.0) [0.36264872550964355s]\n","Episode: 490, Score: 10.0 (Running Avg:10.0) [0.6225168704986572s]\n","Episode: 500, Score: 10.0 (Running Avg:10.0) [0.7458059787750244s]\n","Episode: 510, Score: 10.0 (Running Avg:9.0) [0.931910514831543s]\n","Episode: 520, Score: 10.0 (Running Avg:10.0) [0.47040462493896484s]\n","Episode: 530, Score: 10.0 (Running Avg:10.0) [0.5489692687988281s]\n","Episode: 540, Score: 10.0 (Running Avg:10.0) [0.7080838680267334s]\n","Episode: 550, Score: 10.0 (Running Avg:10.0) [0.6441018581390381s]\n","Episode: 560, Score: 10.0 (Running Avg:10.0) [0.6997466087341309s]\n","Episode: 570, Score: 10.0 (Running Avg:10.0) [0.4414687156677246s]\n","Episode: 580, Score: 10.0 (Running Avg:10.0) [0.5863204002380371s]\n","Episode: 590, Score: 10.0 (Running Avg:10.0) [0.4159536361694336s]\n","Episode: 600, Score: 10.0 (Running Avg:10.0) [0.17504048347473145s]\n","Episode: 610, Score: 10.0 (Running Avg:10.0) [0.6513128280639648s]\n","Episode: 620, Score: 10.0 (Running Avg:10.0) [0.1838850975036621s]\n","Episode: 630, Score: 10.0 (Running Avg:10.0) [0.35425591468811035s]\n","Episode: 640, Score: 10.0 (Running Avg:10.0) [0.31641435623168945s]\n","Episode: 650, Score: 10.0 (Running Avg:10.0) [0.25524353981018066s]\n","Episode: 660, Score: 10.0 (Running Avg:10.0) [0.3913552761077881s]\n","Episode: 670, Score: 10.0 (Running Avg:10.0) [0.17450904846191406s]\n","Episode: 680, Score: 10.0 (Running Avg:10.0) [0.5809316635131836s]\n","Episode: 690, Score: 10.0 (Running Avg:10.0) [0.3257880210876465s]\n","Episode: 700, Score: 10.0 (Running Avg:10.0) [0.2488880157470703s]\n","Episode: 710, Score: 10.0 (Running Avg:10.0) [0.5198943614959717s]\n","Episode: 720, Score: 10.0 (Running Avg:10.0) [0.28208065032958984s]\n","Episode: 730, Score: 10.0 (Running Avg:10.0) [0.40495991706848145s]\n","Episode: 740, Score: 10.0 (Running Avg:10.0) [0.24326753616333008s]\n","Episode: 750, Score: 10.0 (Running Avg:10.0) [0.2867302894592285s]\n","Episode: 760, Score: 10.0 (Running Avg:10.0) [0.4015631675720215s]\n","Episode: 770, Score: 10.0 (Running Avg:10.0) [0.3378007411956787s]\n","Episode: 780, Score: 10.0 (Running Avg:10.0) [0.30983781814575195s]\n","Episode: 790, Score: 10.0 (Running Avg:10.0) [0.3251619338989258s]\n","Episode: 800, Score: 10.0 (Running Avg:10.0) [0.2007138729095459s]\n","Episode: 810, Score: 10.0 (Running Avg:10.0) [0.22238683700561523s]\n","Episode: 820, Score: 10.0 (Running Avg:10.0) [0.15650272369384766s]\n","Episode: 830, Score: 10.0 (Running Avg:10.0) [0.23957252502441406s]\n","Episode: 840, Score: 10.0 (Running Avg:10.0) [0.22358179092407227s]\n","Episode: 850, Score: 10.0 (Running Avg:10.0) [0.3158531188964844s]\n","Episode: 860, Score: 10.0 (Running Avg:10.0) [0.29601454734802246s]\n","Episode: 870, Score: 10.0 (Running Avg:10.0) [0.18527650833129883s]\n","Episode: 880, Score: 10.0 (Running Avg:10.0) [0.2065422534942627s]\n","Episode: 890, Score: 10.0 (Running Avg:10.0) [0.22114276885986328s]\n","Episode: 900, Score: 10.0 (Running Avg:10.0) [0.21920108795166016s]\n","Episode: 910, Score: 10.0 (Running Avg:10.0) [0.19783258438110352s]\n","Episode: 920, Score: 10.0 (Running Avg:10.0) [0.13198161125183105s]\n","Episode: 930, Score: 10.0 (Running Avg:10.0) [0.28328585624694824s]\n","Episode: 940, Score: 10.0 (Running Avg:10.0) [0.2178804874420166s]\n","Episode: 950, Score: 10.0 (Running Avg:10.0) [0.25655317306518555s]\n","Episode: 960, Score: 10.0 (Running Avg:10.0) [0.2520887851715088s]\n","Episode: 970, Score: 10.0 (Running Avg:10.0) [0.2071375846862793s]\n","Episode: 980, Score: 10.0 (Running Avg:10.0) [0.20007562637329102s]\n","Episode: 990, Score: 10.0 (Running Avg:10.0) [0.2895534038543701s]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":265},"id":"Gcz2ZBMDyIqp","executionInfo":{"status":"ok","timestamp":1616080022037,"user_tz":240,"elapsed":596,"user":{"displayName":"Annik Carson","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghrwy2SzOHtgbFT4k-SP4JNYSMBAVnHobU7WyaqVg=s64","userId":"13111220454514414588"}},"outputId":"cda1d39f-aa84-4c93-bfea-0f6cad2188bc"},"source":["plt.figure()\n","plt.plot(ex.data['total_reward'])\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"IYekC_hL0L0w"},"source":[""],"execution_count":null,"outputs":[]}]}