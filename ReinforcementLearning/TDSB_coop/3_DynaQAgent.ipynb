{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4 - Dyna Q Agent.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMGNAWWGhD7j7hRKY+igVpm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"egClwonvx_Jb"},"source":["In progress ** \n","\n","Model based reinforcement learning in a gridworld environment using a model-based Dyna-Q agent. \n","\n","Based on code by Kirth Sathiyakumar and Annik Carson\n","\n","---\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OAhJVCgos1ZC","executionInfo":{"status":"ok","timestamp":1615944172433,"user_tz":240,"elapsed":26340,"user":{"displayName":"Annik Carson","photoUrl":"","userId":"09001828844446457907"}},"outputId":"53cccbdb-7074-42af-f331-9d25c9b3b369"},"source":["# import relevant packages\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from pylab import *\n","\n","# we have offloaded our classes defined previously to py files which we need to import here to use in this notebook \n","from google.colab import drive\n","drive.mount('/content/drive')\n","import sys\n","\n","sys.path.insert(0,'/content/drive/My Drive/Colab Notebooks/CoOp Project/') ## will have to change this path! \n","\n","# import premade Gridworlds from Week2 Tutorial\n","from pyfiles.GridworldEnvs import BoundedGridworld, EdgewrapGridworld, CliffWorld\n","\n","# import utility functions\n","from pyfiles.Utils import running_mean as rm\n","from pyfiles.Utils import oneD2twoD"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"muhe58Zgxmh8","executionInfo":{"status":"ok","timestamp":1615944210001,"user_tz":240,"elapsed":460,"user":{"displayName":"Annik Carson","photoUrl":"","userId":"09001828844446457907"}}},"source":["class DynaAgent:\n","    \n","    def __init__(self, exp_rate=0.1, lr=0.1, n_steps=50, episodes=1):\n","        \n","        self.maze = env\n","        self.state = env.state1D\n","        self.actions = env.action_list\n","        self.state_actions = []  # state & action track\n","        self.exp_rate = 0.3\n","        self.lr = 0.1\n","        \n","        self.steps = n_steps\n","        self.episodes = episodes  # number of episodes going to play\n","        self.steps_per_episode = []\n","        self.rwd_tracker = []\n","        self.Q_values = {}\n","        \n","        # model function\n","        self.model = {}\n","        for row in range(env.r):\n","            for col in range(env.c):\n","                self.Q_values[(row, col)] = {}\n","                for a in self.actions:\n","                    self.Q_values[(row, col)][a] = 0\n","        \n","    def chooseAction (self):\n","        # Epsilon-greedy agent policy\n","        state = env.state1D\n","        state2d = env.state2D\n","        actions_allowed = env.get_actions()\n","        \n","        if random.uniform(0, 1) <= self.exp_rate:\n","            # explore\n","            choose = np.asarray([*env.action_dict.values()])[actions_allowed]\n","            choose_1 = np.asarray(env.action_list)[actions_allowed]\n","            choice = np.random.choice(choose)\n","            return (choice)\n","\n","        else:\n","            # exploit on allowed actions\n","            actions_allowed = np.asarray([*env.action_dict.values()])[actions_allowed]\n","            Q_s = self.Q_values[state2d]\n","            actions_greedy_1 = actions_allowed[np.flatnonzero(Q_s == np.max(Q_s))]\n","            choice_number = np.random.choice(actions_greedy_1)\n","            return (choice_number)\n","    \n","    def reset(self):\n","        self.maze = env\n","        self.state = env.state1D\n","        self.state_actions = []\n","        \n","    def play_dummy(self):\n","        self.reset()\n","        rwd_tracker = []\n","        \n","        for ep in range(self.episodes):\n","            #reset env to baseline \n","            env.resetEnvironment()\n","            \n","            #reset cumulative reward to 0 \n","            cum_rwd = 0\n","            \n","            #step through          \n","            for step in range (self.steps):\n","                #get state\n","                state2D = env.state2D\n","                \n","                #select action\n","                action = self.chooseAction()\n","                action_str = self.actions[action]\n","                \n","                #move\n","                nxtState, reward, done = env.move(action)\n","                #print(reward)\n","                \n","                #update q dictionary \n","                self.Q_values[state2D][action_str] += self.lr*(reward + np.max(list(self.Q_values[nxtState].values())) - self.Q_values[state2D][action_str])\n","                \n","                #add reward to counter \n","                cum_rwd += reward \n","                \n","            rwd_tracker.append(cum_rwd)\n","            \n","            if ep % 50 == 0:\n","                print(f\"Episode:{ep} Rewards:{cum_rwd}\")\n","    \n","    def play(self, n_timesteps,print_frequency=25): \n","        #reseting for fresh run \n","        self.reset()\n","        self.rwd_tracker = []\n","        \n","        for ep in range(self.episodes):    \n","            env.resetEnvironment()# starting state\n","            cum_rwd = 0 #reset cumulative reward to 0 \n","            state1D = env.state1D\n","            state2D = env.state2D\n","            \n","            for time_steps in range(n_timesteps):\n","                #choose action and take a step in the env \n","                action = self.chooseAction()\n","                action_str = self.actions[action]\n","                nxtState, reward, done = env.move(action)\n","                \n","                # update Q-value\n","                self.Q_values[state2D][action_str] += self.lr*(reward + np.max(list(self.Q_values[nxtState].values())) - self.Q_values[state2D][action_str])\n","                \n","                #track reward \n","                cum_rwd += reward\n","                \n","                # add transition to model\n","                if state2D not in self.model.keys():\n","                    self.model[state2D] = {}\n","                self.model[state2D][action_str] = (reward, nxtState)\n","                self.state = nxtState\n","            \n","            #add total episode reward     \n","            self.rwd_tracker.append(cum_rwd)\n","\n","            # update Q-values based on planning from memory \n","            for step_number in range(self.steps):\n","                # randomly choose an state\n","                rand_idx = np.random.choice(range(len(self.model.keys()))) #randomly sample one state from the model dict\n","                _state = list(self.model)[rand_idx] \n","\n","                # randomly choose an action\n","                rand_idx = np.random.choice(range(len(self.model[_state].keys()))) #randomly sample one action from the state dict of the model \n","                _action_str = list(self.model[_state])[rand_idx]\n","                _reward, _nxtState = self.model[_state][_action_str]\n","                #update the q value based on this random sample from memory\n","                self.Q_values[_state][_action_str] += self.lr*(_reward + np.max(list(self.Q_values[_nxtState].values())) - self.Q_values[_state][_action_str])       \n","\n","\n","            if ep % print_frequency == 0:\n","                print(f\"Episode:{ep} Rewards:{sum(self.rwd_tracker[-print_frequency:])}\")\n","    \n","        return self.rwd_tracker\n","    "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"paNA_wTFx9Qq"},"source":[""],"execution_count":null,"outputs":[]}]}